{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import deque\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import sample, choice\n",
    "\n",
    "from gym import Env, make\n",
    "from gym.spaces import Discrete, Box\n",
    "from gym.utils import seeding\n",
    "from IPython.display import HTML\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "IMAGE_SIZE = 4\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "\n",
    "    STATE_INIT                 = 0\n",
    "    STATE_COLLECTED            = 1\n",
    "    STATE_MOVED                = 2\n",
    "    STATE_MOVED_AND_HIT_BORDER = 3\n",
    "    STATE_WON                  = 4\n",
    "    \n",
    "    UP    = TOP    = 0\n",
    "    RIGHT          = 1\n",
    "    DOWN  = BOTTOM = 2\n",
    "    LEFT           = 3\n",
    "    \n",
    "    X = 0\n",
    "    Y = 1\n",
    "\n",
    "    bd_colors = {\n",
    "        \"BLACK\": 0,\n",
    "        \"GREEN\": 1,\n",
    "        \"BLUE\" : 2\n",
    "    }\n",
    "\n",
    "    im_colors = {\n",
    "        \"BLACK\": np.array([0, 0, 0]),\n",
    "        \"GREEN\": np.array([0, 255, 0]),\n",
    "        \"BLUE\" : np.array([0, 0, 255])\n",
    "    }\n",
    "\n",
    "    def __init__(self, img_size=4, collectibles=4):\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.collectibles_count = collectibles\n",
    "        self.collectibles_left = collectibles\n",
    "        self.im = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)\n",
    "        self.bd = np.zeros((self.img_size, self.img_size), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):\n",
    "        self.color_hit_after_move = \"BLACK\"\n",
    "\n",
    "        self.collectibles_left = self.collectibles_count\n",
    "\n",
    "        self.last_player_pos = [0, 0]\n",
    "        self.player_pos      = self.last_player_pos.copy()\n",
    "\n",
    "        self.game_state = Game.STATE_INIT\n",
    "\n",
    "        self.bd = np.zeros((self.img_size, self.img_size), dtype=np.uint8)\n",
    "        self.im = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)\n",
    "\n",
    "        self.__set_color(self.player_pos, \"BLUE\")\n",
    "\n",
    "        self.__generate_collectibles()\n",
    "\n",
    "        return self.get_frame()\n",
    "\n",
    "    def __set_color(self, p, c):\n",
    "        self.bd[p[0]][p[1]] = Game.bd_colors[c]\n",
    "        self.im[p[0]][p[1]] = Game.im_colors[c]\n",
    "\n",
    "    def __get_color(self, p):\n",
    "        return self.bd[p[0]][p[1]]\n",
    "\n",
    "    def __hit_border(self, border):\n",
    "\n",
    "        hit = False\n",
    "\n",
    "        if border == Game.TOP:\n",
    "            if self.player_pos[Game.X] - 1 < 0:\n",
    "                hit = True\n",
    "\n",
    "        elif border == Game.RIGHT:\n",
    "            if self.player_pos[Game.Y] + 1 > self.img_size - 1:\n",
    "                hit = True\n",
    "\n",
    "        elif border == Game.BOTTOM:\n",
    "            if self.player_pos[Game.X] + 1 > self.img_size - 1:\n",
    "                hit = True\n",
    "\n",
    "        elif border == Game.LEFT:\n",
    "            if self.player_pos[Game.Y] - 1 < 0:\n",
    "                hit = True\n",
    "\n",
    "        return hit\n",
    "\n",
    "    def move_player(self, direction):\n",
    "\n",
    "        hit_border = self.__hit_border(direction)\n",
    "\n",
    "        if not hit_border:\n",
    "\n",
    "            self.last_player_pos[Game.X] = self.player_pos[Game.X]\n",
    "            self.last_player_pos[Game.Y] = self.player_pos[Game.Y]\n",
    "\n",
    "            if direction == Game.UP:\n",
    "                self.player_pos[Game.X] -= 1\n",
    "\n",
    "            elif direction == Game.RIGHT:\n",
    "                self.player_pos[Game.Y] += 1\n",
    "\n",
    "            elif direction == Game.DOWN:\n",
    "                self.player_pos[Game.X] += 1\n",
    "\n",
    "            elif direction == Game.LEFT:\n",
    "                self.player_pos[Game.Y] -= 1\n",
    "\n",
    "        self.__update_game(hit_border)\n",
    "\n",
    "    def get_game_state(self):\n",
    "        return self.game_state\n",
    "\n",
    "    def get_frame(self, image=False):\n",
    "        if image:\n",
    "            return self.im.copy()\n",
    "        \n",
    "        return self.bd.copy()\n",
    "\n",
    "    def __generate_collectibles(self):\n",
    "\n",
    "        avail_pos = [\n",
    "            pos\n",
    "            for pos in np.ndindex(self.img_size, self.img_size)\n",
    "            if pos != (0, 0)\n",
    "        ]\n",
    "\n",
    "        for idx in range(self.collectibles_count):\n",
    "            pos = avail_pos.pop(avail_pos.index(choice(avail_pos)))\n",
    "            self.__set_color(pos, \"GREEN\")\n",
    "\n",
    "    def __update_game(self, hit_border):\n",
    "        self.color_hit_after_move = self.__get_color(self.player_pos)\n",
    "\n",
    "        if hit_border:\n",
    "            self.game_state = Game.STATE_MOVED_AND_HIT_BORDER\n",
    "            \n",
    "        elif not hit_border:\n",
    "            if self.color_hit_after_move == self.bd_colors[\"BLACK\"]:\n",
    "                self.game_state = Game.STATE_MOVED\n",
    "                \n",
    "            elif self.color_hit_after_move == self.bd_colors[\"GREEN\"]:\n",
    "                self.game_state = Game.STATE_COLLECTED\n",
    "                \n",
    "                self.collectibles_left -= 1\n",
    "                \n",
    "                if(self.collectibles_left <= 0):\n",
    "                    self.game_state = Game.STATE_WON\n",
    "\n",
    "        self.__set_color(self.player_pos, \"BLUE\")\n",
    "        self.__set_color(self.last_player_pos, \"BLACK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(Env):\n",
    "\n",
    "    def __init__(self, img_size=IMAGE_SIZE):\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Box(\n",
    "           low=0,\n",
    "           high=255,\n",
    "           shape=(img_size, img_size, 3),\n",
    "           dtype=np.uint8\n",
    "        )\n",
    "        self.done = False\n",
    "        self.info = {}\n",
    "\n",
    "        self.g = Game()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.obs = self.g.get_frame()\n",
    "\n",
    "        self.g.move_player(action)\n",
    "\n",
    "        game_state = self.g.get_game_state()\n",
    "\n",
    "        if game_state == self.g.STATE_COLLECTED:\n",
    "            self.reward = 20\n",
    "\n",
    "        elif game_state == self.g.STATE_MOVED:\n",
    "            self.reward = 5\n",
    "\n",
    "        elif game_state == self.g.STATE_MOVED_AND_HIT_BORDER:\n",
    "            self.reward = 0\n",
    "\n",
    "        elif game_state == self.g.STATE_WON:\n",
    "            self.reward = 50\n",
    "            self.done = True\n",
    "\n",
    "        return self.obs, self.reward, self.done, self.info\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.g.reset()\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        self.info = {}\n",
    "        return obs\n",
    "\n",
    "    def render(self):\n",
    "        return self.g.get_frame(True)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, size, data_type):\n",
    "\n",
    "        self.memory = deque(maxlen=size)\n",
    "        self.data_type = data_type\n",
    "\n",
    "    def save(self, content):\n",
    "        assert isinstance(content, self.data_type)\n",
    "        self.memory.append(content)\n",
    "\n",
    "    def load(self):\n",
    "        return self.memory\n",
    "\n",
    "    def erase(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "        \n",
    "class Agent:\n",
    "    def __init__(self, input_shape, env):\n",
    "        self.__env = env\n",
    "\n",
    "        self.__hparams = {\n",
    "            \"EPSILON\": 1.0,          # Exploration factor\n",
    "            \"EPSILON_DECAY\": 0.995,   # Exploration factor discount\n",
    "            \"EPSILON_LIMIT\": 0.01,   # Exploration factor min value\n",
    "            \"GAMMA\": 0.99,           # Discount Rate for Bellman's equation\n",
    "            \"LEARNING_RATE\": 0.001,  # Learning Rate for DNN optimizer\n",
    "            \"TAU\": 0.125             # Target model training interval\n",
    "        }\n",
    "\n",
    "        self.__memory = Memory(2000, tuple)\n",
    "\n",
    "        self.__input_shape = input_shape\n",
    "        self.__model = self.__build_model()\n",
    "        self.__target_model = self.__build_model()\n",
    "        \n",
    "        print(self.__model.summary())\n",
    "\n",
    "    def choose_action(self, state):\n",
    "\n",
    "        # Apply discount to exploration rate\n",
    "        self.__hparams[\"EPSILON\"] *= self.__hparams[\"EPSILON_DECAY\"]\n",
    "\n",
    "        # If exploration rate reached or exceeded limit, set it to min\n",
    "        self.__hparams[\"EPSILON\"] = max(\n",
    "            self.__hparams[\"EPSILON_LIMIT\"], self.__hparams[\"EPSILON\"]\n",
    "        )\n",
    "\n",
    "        # Each time, it gets harder to explore and easier to exploit\n",
    "        # If algorithm is exploring, return a random action\n",
    "        if np.random.rand() < self.__hparams[\"EPSILON\"]:\n",
    "            return self.__env.action_space.sample()\n",
    "\n",
    "        # Else, predict Q-values (future discounted rewards for each action) for current state\n",
    "        tf_state = np.expand_dims(state, axis=0)\n",
    "        Q_sa = self.__model.predict(tf_state)[0]\n",
    "\n",
    "        # Return index of the highest Q-value\n",
    "        return np.argmax(Q_sa)\n",
    "\n",
    "#     def play(self, batch_size):\n",
    "\n",
    "#         if len(self.__memory.load()) < batch_size:\n",
    "#             return\n",
    "\n",
    "#         # Get <batch_size> random memory entries\n",
    "#         batch = sample(self.__memory.load(), batch_size)\n",
    "\n",
    "#         for state, action, reward, new_state, done in batch:\n",
    "\n",
    "#             # TensorFlow requires an extra dimension specifing # of inputs\n",
    "#             tf_state = np.expand_dims(state, axis=0)\n",
    "#             tf_new_state = np.expand_dims(new_state, axis=0)\n",
    "\n",
    "#             Q_sa = self.__target_model.predict(tf_state)\n",
    "\n",
    "#             if done:\n",
    "#                 # Game ended, our target is the current state reward\n",
    "#                 Q_sa[0][action] = reward\n",
    "#             else:\n",
    "#                 # Game still running, our target is the current state reward\n",
    "#                 # plus next state discounted reward (Bellman's equation)\n",
    "#                 Q_sa[0][action] = reward + self.__hparams[\"GAMMA\"] \\\n",
    "#                     * np.max(self.__target_model.predict(tf_new_state)[0])\n",
    "\n",
    "#             self.__model.fit(tf_state, Q_sa, epochs=1, verbose=0)\n",
    "\n",
    "    def play(self, batch_size):\n",
    "\n",
    "        if len(self.__memory.load()) < batch_size:\n",
    "            return\n",
    "\n",
    "        # Get <batch_size> random memory entries\n",
    "        batch = sample(self.__memory.load(), batch_size)\n",
    "        states = np.zeros((batch_size, state.shape[1:]))\n",
    "        target = np.zeros((batch_size, self.env.action_space.n))\n",
    "        \n",
    "        i = 0\n",
    "        for state, action, reward, new_state, done in batch:\n",
    "            \n",
    "            # TensorFlow requires an extra dimension specifing # of inputs\n",
    "            tf_state = np.expand_dims(state, axis=0)\n",
    "            tf_new_state = np.expand_dims(new_state, axis=0)\n",
    "\n",
    "            states[i] = tf_state\n",
    "            target_y  = self.__model.predict(tf_state)[0]\n",
    "            Q_sa      = self.__model.predict(tf_new_state)[0]\n",
    "            \n",
    "            print(states)\n",
    "            print(target_y)\n",
    "            print(Q_sa)\n",
    "\n",
    "            if done:\n",
    "                # Game ended, our target is the current state reward\n",
    "                target_y[i][action] = reward\n",
    "            else:\n",
    "                # Game still running, our target is the current state reward\n",
    "                # plus next state discounted reward (Bellman's equation)\n",
    "                target_y[i][action] = reward + self.__hparams[\"GAMMA\"] * np.max(Q_sa)\n",
    "\n",
    "            self.__model.fit(states, target_y, epochs=1, verbose=0)\n",
    "\n",
    "    def record(self, content):\n",
    "        self.__memory.save(content)\n",
    "\n",
    "#     def target_train(self):\n",
    "#         # Copy weights from main model to target model\n",
    "\n",
    "#         model_weights = self.__model.get_weights()\n",
    "#         target_model_weights = self.__target_model.get_weights()\n",
    "#         tau = self.__hparams[\"TAU\"]\n",
    "\n",
    "#         for i in range(len(target_model_weights)):\n",
    "#             target_model_weights[i] = \\\n",
    "#                 model_weights[i] * tau + target_model_weights[i] * \\\n",
    "#                 (1 - tau)\n",
    "\n",
    "#         self.__target_model.set_weights(target_model_weights)\n",
    "\n",
    "    def __build_model(self):\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        l_dens1 = layers.Dense(32, activation=\"relu\", input_shape=self.__input_shape)\n",
    "        l_dens2 = layers.Dense(32, activation=\"relu\")\n",
    "        l_flat1 = layers.Flatten()\n",
    "        l_dens3 = layers.Dense(self.__env.action_space.n, activation=\"sigmoid\")\n",
    "\n",
    "        model.add(l_dens1)\n",
    "        model.add(l_dens2)\n",
    "        model.add(l_flat1)\n",
    "        model.add(l_dens3)\n",
    "\n",
    "        opt = optimizers.Adam(lr=self.__hparams[\"LEARNING_RATE\"])\n",
    "\n",
    "        model.compile(loss=\"mse\", optimizer=opt)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def load(self, name):\n",
    "        self.__model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.__model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4, 32)             160       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4, 32)             1056      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,732\n",
      "Trainable params: 1,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'state' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6a68dddec5b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Train main model based on recorded states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update target model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4cf9776d00a8>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Get <batch_size> random memory entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'state' referenced before assignment"
     ]
    }
   ],
   "source": [
    "EPISODES = 100\n",
    "STEPS = 40\n",
    "INPUT_SHAPE = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "env = MyEnv()\n",
    "\n",
    "agent = Agent(INPUT_SHAPE, env)\n",
    "\n",
    "# Play <EPISODES> games\n",
    "for ep in range(EPISODES):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Each game takes <STEPS> steps\n",
    "    for st in range(STEPS):\n",
    "\n",
    "        # Get an action and execute it in the environment\n",
    "        action = agent.choose_action(state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store experience acquired by the \"player\"\n",
    "        # after executing an action\n",
    "        agent.record((state, action, reward, new_state, done))\n",
    "\n",
    "        # Train main model based on recorded states\n",
    "        agent.play(BATCH_SIZE)\n",
    "\n",
    "        # Update target model\n",
    "#         agent.target_train()\n",
    "\n",
    "        state = new_state.copy()\n",
    "\n",
    "        if done:\n",
    "            agent.save(\"weights.h5\")\n",
    "            break\n",
    "\n",
    "agent.save(\"weights.h5\")\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    agent.load(\"weights.h5\")\n",
    "except (OSError, ValueError) as e:\n",
    "    exit(e)\n",
    "\n",
    "STEPS = 40\n",
    "    \n",
    "frames = []\n",
    "obs = env.reset()\n",
    "frames.append(env.render())\n",
    "\n",
    "for st in range(STEPS):\n",
    "    action = agent.choose_action(obs)\n",
    "\n",
    "    new_obs, reward, done, info = env.step(action)\n",
    "    frames.append(env.render())\n",
    "\n",
    "    obs = new_obs.copy()\n",
    "    \n",
    "    if done:\n",
    "        print(\"DONE\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create gameplay GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ANIMATION_INTERVAL = 200\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "def update(i):\n",
    "    frame = frames[i]\n",
    "    ax.imshow(frame)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update, frames=len(frames), interval=ANIMATION_INTERVAL)\n",
    "anim.save('play.gif', dpi=80, writer='imagemagick')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Click here to view generated GIF](./play.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
